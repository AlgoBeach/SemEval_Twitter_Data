{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "\n",
    "\n",
    "from ttp import ttp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"data/processed/gold/train/100_topics_100_tweets.sentence-three-point.subtask-A.train.gold.txt\"\n",
    "test_file = \"data/processed/gold/dev/100_topics_100_tweets.sentence-three-point.subtask-A.dev.gold.txt\"\n",
    "df_train = pd.read_csv(train_file, header=None, sep=\"\\t\")\n",
    "df_test = pd.read_csv(test_file, header= None, sep=\"\\t\")\n",
    "NULL_TEXT = \"Not Available\"\n",
    "df_text_train = df_train[df_train[2] != NULL_TEXT][[1,2]]\n",
    "df_text_test = df_test[df_test[2] != NULL_TEXT][[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>If I make a game as a #windows10 Universal App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>Microsoft, I may not prefer your gaming branch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>@MikeWolf1980 @Microsoft I will be downgrading...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1                                                  2\n",
       "0  negative  dear @Microsoft the newOoffice for Mac is grea...\n",
       "1  negative  @Microsoft how about you make a system that do...\n",
       "4   neutral  If I make a game as a #windows10 Universal App...\n",
       "5  positive  Microsoft, I may not prefer your gaming branch...\n",
       "6  negative  @MikeWolf1980 @Microsoft I will be downgrading..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = ttp.Parser(include_spans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USER_TAG = \"_USER\"\n",
    "HASH_TAG = \"_HASHTAG\"\n",
    "URL_TAG = \"_URL\"\n",
    "LISTS_TAG = \"_LIST\"\n",
    "\n",
    "\n",
    "def preprocess(item):\n",
    "    txt = item[2]\n",
    "    cols = [\"label\", \"text\", \"norm_text\", \"c_url\", \"c_user\", \"c_tag\", \"c_list\", \"is_reply\",\"t_len\"]\n",
    "    result = p.parse(txt)\n",
    "    counts = [len(result.urls), len(result.users), len(result.tags), len(result.lists), result.reply is not None, len(txt)]\n",
    "    processed_txt = \"\"\n",
    "    last = 0\n",
    "    all_results = map(lambda x: (x[0], USER_TAG, x[1]), result.users) +\\\n",
    "        map(lambda x: (x[0], HASH_TAG, x[1]), result.tags) + map(lambda x: (x[0], URL_TAG, x[1]), result.urls) +\\\n",
    "        map(lambda x: (x[0], LISTS_TAG, x[1]), result.lists)\n",
    "    all_results = sorted(all_results, key=lambda x: x[2][1])\n",
    "    for k, t, v in all_results:\n",
    "        processed_txt += txt[last:v[0]] + t\n",
    "        last = v[1]\n",
    "    #return all_results, counts, processed_txt\n",
    "    return pd.Series([item[1], item[2], processed_txt] + counts, index=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>norm_text</th>\n",
       "      <th>c_url</th>\n",
       "      <th>c_user</th>\n",
       "      <th>c_tag</th>\n",
       "      <th>c_list</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>t_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "      <td>dear _USER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "      <td>_USER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>If I make a game as a #windows10 Universal App...</td>\n",
       "      <td>If I make a game as a _HASHTAG Universal App. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>Microsoft, I may not prefer your gaming branch...</td>\n",
       "      <td>Microsoft, I may not prefer your gaming branch...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>@MikeWolf1980 @Microsoft I will be downgrading...</td>\n",
       "      <td>_USER _USER I will be downgrading and let #Win...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0  negative  dear @Microsoft the newOoffice for Mac is grea...   \n",
       "1  negative  @Microsoft how about you make a system that do...   \n",
       "4   neutral  If I make a game as a #windows10 Universal App...   \n",
       "5  positive  Microsoft, I may not prefer your gaming branch...   \n",
       "6  negative  @MikeWolf1980 @Microsoft I will be downgrading...   \n",
       "\n",
       "                                           norm_text  c_url  c_user  c_tag  \\\n",
       "0                                         dear _USER      0       1      0   \n",
       "1                                              _USER      0       1      0   \n",
       "4  If I make a game as a _HASHTAG Universal App. ...      0       2      2   \n",
       "5  Microsoft, I may not prefer your gaming branch...      0       1      1   \n",
       "6  _USER _USER I will be downgrading and let #Win...      0       2      2   \n",
       "\n",
       "   c_list is_reply  t_len  \n",
       "0       0    False     83  \n",
       "1       0     True    136  \n",
       "4       0    False    137  \n",
       "5       0    False    128  \n",
       "6       0     True    129  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_train.head().apply(preprocess, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (5366, 9) (1798, 9)\n"
     ]
    }
   ],
   "source": [
    "df_text_train_proc = df_text_train.apply(preprocess, axis=1)\n",
    "df_text_test_proc = df_text_test.apply(preprocess, axis=1)\n",
    "\n",
    "\n",
    "print df_text_train_proc.shape, df_text_test_proc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = df_text_train_proc.columns.values\n",
    "\n",
    "class ColumnFeatures(TransformerMixin):\n",
    "    def __init__(self, colname, to_df=True):\n",
    "        print \"Initialized extractor for column %s\" % colname\n",
    "        self.colname = colname\n",
    "        self.to_df = to_df\n",
    "    def get_feature_names(self):\n",
    "        return [self.colname]\n",
    "    def transform(self, X, **transform_params):\n",
    "        print \"Extracting column [%s], to_df = %s\" % (self.colname, self.to_df)\n",
    "        if self.to_df:\n",
    "            return pd.DataFrame(X[self.colname])\n",
    "        return X[self.colname]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        print \"New shape: \",  X.todense().shape\n",
    "        return X.todense()\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class IdentityTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        print \"X processed by parent. Output shape: %s\" % (X.shape, )\n",
    "        return X\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class MultiColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, colnames, to_series=False):\n",
    "        print \"Initialized extractor for column %s\" % colnames\n",
    "        self.colnames = colnames\n",
    "        self.to_series = to_series\n",
    "    def get_feature_names(self):\n",
    "        return self.colnames\n",
    "    def transform(self, X, **transform_params):\n",
    "        print \"Extracting columns [%s] from X with shape: %s\" % (self.colnames,X.shape)\n",
    "        if self.to_series:\n",
    "            if len(self.colnames) != 1:\n",
    "                raise Exception(\"When to_series is True, len(colnames) should be 1\")\n",
    "            return X[self.colnames[0]]\n",
    "        return pd.DataFrame(X[self.colnames])\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    \n",
    "class DictScoreExtractor(TransformerMixin):\n",
    "    def __init__(self, colname, senti_dict, pattern=r'[^a-zA-Z]+'):\n",
    "        print \"Initialized extractor for column %s with pattern %s\" % (colname, pattern)\n",
    "        self.colname = colname\n",
    "        self.pattern = re.compile(pattern)\n",
    "        self.feature_names = [\"tot_pos\", \"tot_neg\"]\n",
    "        self.senti_dict = senti_dict\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    def _extract_features(self,x):\n",
    "        tokens = re.split(self.pattern, x.strip())\n",
    "        pos_score, neg_score = 0.0, 0.0\n",
    "        for t in tokens:\n",
    "            p, n = self.senti_dict.get(t, [0.,0.])\n",
    "            pos_score += p\n",
    "            neg_score += n\n",
    "        return pd.Series([pos_score, neg_score])\n",
    "    def transform(self, X, **transform_params):\n",
    "        print \"Extracting columns [%s] from X with shape: %s\" % (self.colname,X.shape)\n",
    "        X_sc = X[self.colname].apply(self._extract_features)\n",
    "        return pd.DataFrame(X_sc)\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict lookup [ 0.  0.]\n",
      "Initialized extractor for column norm_text with pattern [^a-zA-Z]+\n",
      "Extracting columns [norm_text] from X with shape: (5, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364583</td>\n",
       "      <td>0.21875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1\n",
       "0  0.000000  0.00000\n",
       "1  0.733333  0.81250\n",
       "2  0.364583  0.21875\n",
       "4  0.000000  0.00000\n",
       "5  0.000000  0.00000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Dict lookup\", swn_dict.get(\"\")\n",
    "\n",
    "d = DictScoreExtractor(\"norm_text\", swn_dict)\n",
    "\n",
    "\n",
    "d.fit_transform(df_text_test_proc.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized extractor for column norm_text with pattern [^a-zA-Z]+\n",
      "Initialized extractor for column ['c_url' 'c_user' 'c_tag' 'c_list' 'is_reply' 't_len']\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "        (\"features\", Pipeline([\n",
    "                    (\"union_feature\", FeatureUnion([\n",
    "                                #(\"text\", Pipeline([\n",
    "                                #            (\"norm_txt\", MultiColumnExtractor([\"norm_text\"], to_series=True)),\n",
    "                                #            (\"count_feature\", CountVectorizer(min_df = 3, stop_words=\"english\",\\\n",
    "                                #                                              ngram_range=(1,1), binary=True)),\n",
    "                                #        ])),\n",
    "                                (\"dict_score\", Pipeline([\n",
    "                                            (\"dict_scores\", DictScoreExtractor(\"norm_text\", swn_dict)),\n",
    "                                            (\"std_d\", StandardScaler()),\n",
    "                                        ])),\n",
    "                                (\"meta_feature\", Pipeline([\n",
    "                                            (\"meta_f\", MultiColumnExtractor(cols[3:])),\n",
    "                                            (\"std_f\", StandardScaler()),\n",
    "                                        ]))\n",
    "                            ]))\n",
    "                ]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting columns [norm_text] from X with shape: (7164, 9)\n",
      "Extracting columns [['c_url' 'c_user' 'c_tag' 'c_list' 'is_reply' 't_len']] from X with shape: (7164, 9)\n",
      "(5366, 8) (5366,) (1798, 8) (1798,)\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.concat((df_text_test_proc, df_text_train_proc))\n",
    "X_all = pipeline.fit_transform(df_all)\n",
    "\n",
    "X_train = X_all[:df_text_train_proc.shape[0]]\n",
    "y_train = df_text_train_proc[\"label\"].values\n",
    "X_test = X_all[df_text_train_proc.shape[0]:]\n",
    "y_test = df_text_test_proc[\"label\"].values\n",
    "\n",
    "print X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.00      0.14      0.01        14\n",
      "    neutral       0.00      0.20      0.00         5\n",
      "   positive       1.00      0.51      0.68      5347\n",
      "\n",
      "avg / total       0.99      0.51      0.68      5366\n",
      "\n",
      "(0.51304509877003357, 0.51304509877003357, 0.51304509877003357, None)\n",
      "[[   2    8    4]\n",
      " [   1    1    3]\n",
      " [ 762 1835 2750]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.00      0.33      0.01         3\n",
      "    neutral       0.00      0.00      0.00         1\n",
      "   positive       1.00      0.42      0.60      1794\n",
      "\n",
      "avg / total       1.00      0.42      0.59      1798\n",
      "\n",
      "(0.42380422691879865, 0.42380422691879865, 0.42380422691879871, None)\n",
      "[[  1   1   1]\n",
      " [  0   0   1]\n",
      " [352 681 761]]\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(multi_class=\"crammer_singer\", C=0.5)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_train)\n",
    "print classification_report(y_pred, y_train)\n",
    "print precision_recall_fscore_support(y_pred, y_train, average=\"micro\")\n",
    "print confusion_matrix(y_pred, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "print classification_report(y_pred, y_test)\n",
    "print precision_recall_fscore_support(y_pred, y_test, average=\"micro\")\n",
    "print confusion_matrix(y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sentiwn = pd.read_csv(\"data/SentiWordNet_3.0.0_20130122.txt\", sep=\"\\t\", skiprows=26, usecols=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>PosScore</th>\n",
       "      <th>NegScore</th>\n",
       "      <th>SynsetTerms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>able#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>2098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>unable#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>2312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>dorsal#2 abaxial#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>2527</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>ventral#2 adaxial#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>2730</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>acroscopic#1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  # POS    ID  PosScore  NegScore          SynsetTerms\n",
       "0     a  1740     0.125      0.00               able#1\n",
       "1     a  2098     0.000      0.75             unable#1\n",
       "2     a  2312     0.000      0.00   dorsal#2 abaxial#1\n",
       "3     a  2527     0.000      0.00  ventral#2 adaxial#1\n",
       "4     a  2730     0.000      0.00         acroscopic#1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiwn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117660, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiwn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dictionary with 83119 unigrams. Skipped 68082 non unigrams\n"
     ]
    }
   ],
   "source": [
    "senti_wn_scores = dict()\n",
    "non_unigram_c = 0\n",
    "for row in df_sentiwn.iterrows():\n",
    "    terms = row[1][\"SynsetTerms\"].split()\n",
    "    pos_score, neg_score = row[1][[\"PosScore\", \"NegScore\"]]\n",
    "    #print terms, pos_score, neg_score\n",
    "    for t in terms:\n",
    "        t = t.split(\"#\")[0]\n",
    "        if len(t.split(\"_\")) > 1:\n",
    "            non_unigram_c += 1\n",
    "            continue # Consider only unigram words\n",
    "        if t not in senti_wn_scores:\n",
    "            senti_wn_scores[t] = [0,0,0,0,0]\n",
    "        senti_wn_scores[t][0] += pos_score # Total pos score\n",
    "        senti_wn_scores[t][1] += neg_score # Total neg score\n",
    "        senti_wn_scores[t][2] += (pos_score > 0) # Total pos counts\n",
    "        senti_wn_scores[t][3] += (neg_score > 0) # Total neg counts\n",
    "        senti_wn_scores[t][4] += 1 # Total count\n",
    "print \"Created dictionary with %s unigrams. Skipped %s non unigrams\" % (len(senti_wn_scores), non_unigram_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_swn = pd.DataFrame(senti_wn_scores.values(), index=senti_wn_scores.keys(), columns=[\"pos_score\", \"neg_score\", \"pos_c\", \"neg_c\", \"tot_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_c</th>\n",
       "      <th>neg_c</th>\n",
       "      <th>tot_c</th>\n",
       "      <th>avg_pos_score</th>\n",
       "      <th>avg_neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fawn</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unattackable</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homomorphism</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>underneath</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melosa</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nunnery</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferment</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chthonic</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utnapishtim</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bioko</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>circuitry</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clotted</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonleaded</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acetylate</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hanging</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rhinitis</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hastily</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-reliant</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>localized</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pos_score  neg_score  pos_c  neg_c  tot_c  avg_pos_score  \\\n",
       "fawn              0.250      0.125      2      1      5          0.050   \n",
       "unattackable      0.125      0.000      1      0      1          0.125   \n",
       "homomorphism      0.000      0.000      0      0      1          0.000   \n",
       "underneath        0.000      0.000      0      0      2          0.000   \n",
       "melosa            0.000      0.000      0      0      1          0.000   \n",
       "nunnery           0.000      0.000      0      0      1          0.000   \n",
       "deferment         0.000      0.000      0      0      1          0.000   \n",
       "chthonic          0.000      0.000      0      0      1          0.000   \n",
       "utnapishtim       0.375      0.125      1      1      1          0.375   \n",
       "bioko             0.000      0.000      0      0      1          0.000   \n",
       "circuitry         0.000      0.000      0      0      1          0.000   \n",
       "clotted           0.000      0.000      0      0      1          0.000   \n",
       "nonleaded         0.250      0.000      1      0      1          0.250   \n",
       "acetylate         0.000      0.000      0      0      2          0.000   \n",
       "hanging           0.000      0.000      0      0      3          0.000   \n",
       "woody             0.000      0.000      0      0      3          0.000   \n",
       "rhinitis          0.000      0.500      0      1      1          0.000   \n",
       "hastily           0.125      0.000      1      0      1          0.125   \n",
       "self-reliant      0.250      0.000      1      0      1          0.250   \n",
       "localized         0.000      0.000      0      0      2          0.000   \n",
       "\n",
       "              avg_neg_score  \n",
       "fawn                  0.025  \n",
       "unattackable          0.000  \n",
       "homomorphism          0.000  \n",
       "underneath            0.000  \n",
       "melosa                0.000  \n",
       "nunnery               0.000  \n",
       "deferment             0.000  \n",
       "chthonic              0.000  \n",
       "utnapishtim           0.125  \n",
       "bioko                 0.000  \n",
       "circuitry             0.000  \n",
       "clotted               0.000  \n",
       "nonleaded             0.000  \n",
       "acetylate             0.000  \n",
       "hanging               0.000  \n",
       "woody                 0.000  \n",
       "rhinitis              0.500  \n",
       "hastily               0.000  \n",
       "self-reliant          0.000  \n",
       "localized             0.000  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_swn[[\"avg_pos_score\", \"avg_neg_score\"]] = df_swn[[\"pos_score\", \"neg_score\"]].div(df_swn[\"tot_c\"], axis=0).fillna(0)\n",
    "df_swn.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = pipeline.named_steps[\"features\"].named_steps[\"union_feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvec = f.transformer_list[0][1].named_steps[\"count_feature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = cvec.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'really', u'good', u'book', u'really good', u'good book']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer(\"This is a really good book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=True, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swn_dict = dict(zip(df_swn.index.values, df_swn[[\"avg_pos_score\", \"avg_neg_score\"]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swn_dict[\"woody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
