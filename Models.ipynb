{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7164, 12579) [[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "(5366, 12579) (5366, 3) (1798, 12579) (1798, 3)\n"
     ]
    }
   ],
   "source": [
    "train_file = \"data/processed/gold/train/100_topics_100_tweets.sentence-three-point.subtask-A.train.gold.txt\"\n",
    "test_file = \"data/processed/gold/dev/100_topics_100_tweets.sentence-three-point.subtask-A.dev.gold.txt\"\n",
    "df_train = pd.read_csv(train_file, header=None, sep=\"\\t\")\n",
    "df_test = pd.read_csv(test_file, header= None, sep=\"\\t\")\n",
    "NULL_TEXT = \"Not Available\"\n",
    "df_text_train = df_train[df_train[2] != NULL_TEXT][[1,2]]\n",
    "df_text_test = df_test[df_test[2] != NULL_TEXT][[1,2]]\n",
    "X_text_train, y_text_train = df_text_train[2], df_text_train[1]\n",
    "X_text_test, y_text_test = df_text_test[2], df_text_test[1]\n",
    "\n",
    "\n",
    "X_text = X_text_train.tolist() + X_text_test.tolist()\n",
    "vectorizer = CountVectorizer(min_df=3, ngram_range=(1,3), binary=True)\n",
    "X = vectorizer.fit_transform(X_text).todense()\n",
    "\n",
    "Y_text = y_text_train.tolist() + y_text_test.tolist()\n",
    "lb = LabelBinarizer()\n",
    "Y = lb.fit_transform(Y_text)\n",
    "\n",
    "print X.shape, X[:2]\n",
    "X_train, Y_train = X[0:X_text_train.shape[0]], Y[0:X_text_train.shape[0]]\n",
    "X_test, Y_test = X[X_text_train.shape[0]:], Y[X_text_train.shape[0]:]\n",
    "print X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=X.shape[1], activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5366 samples, validate on 1798 samples\n",
      "Epoch 1/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6761 - acc: 0.7152 - val_loss: 0.9885 - val_acc: 0.5122\n",
      "Epoch 2/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6704 - acc: 0.7164 - val_loss: 0.9920 - val_acc: 0.5100\n",
      "Epoch 3/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6654 - acc: 0.7214 - val_loss: 0.9944 - val_acc: 0.5117\n",
      "Epoch 4/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6604 - acc: 0.7227 - val_loss: 0.9967 - val_acc: 0.5106\n",
      "Epoch 5/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6559 - acc: 0.7259 - val_loss: 1.0000 - val_acc: 0.5095\n",
      "Epoch 6/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6515 - acc: 0.7287 - val_loss: 1.0021 - val_acc: 0.5100\n",
      "Epoch 7/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6473 - acc: 0.7296 - val_loss: 1.0050 - val_acc: 0.5106\n",
      "Epoch 8/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6433 - acc: 0.7324 - val_loss: 1.0087 - val_acc: 0.5106\n",
      "Epoch 9/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6395 - acc: 0.7316 - val_loss: 1.0112 - val_acc: 0.5106\n",
      "Epoch 10/10\n",
      "5366/5366 [==============================] - 0s - loss: 0.6360 - acc: 0.7374 - val_loss: 1.0154 - val_acc: 0.5083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c6fe7ac90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), nb_epoch=10, batch_size=128, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 95\n"
     ]
    }
   ],
   "source": [
    "char_df = pd.read_csv(\"index_char.txt\", sep=\"\\t\", header=None, quoting=3)\n",
    "char_dict = dict(k for k in char_df.values.tolist())\n",
    "index_char = list(k[0] for k in char_df.values.tolist())\n",
    "print len(char_dict), len(index_char)\n",
    "char_vocab_size = len(char_dict) + 2 # 0 for padding and 1 for OOV\n",
    "maxlen = 140 # Tweet length\n",
    "label_dict = dict((k[1], k[0]) for k in enumerate([\"positive\", \"negative\", \"neutral\"]))\n",
    "embedding_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "logit.fit(X_train, y_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.18      0.37      0.25       175\n",
      "    neutral       0.50      0.45      0.47       752\n",
      "   positive       0.65      0.57      0.61       871\n",
      "\n",
      "avg / total       0.54      0.50      0.52      1798\n",
      "\n",
      "(0.50166852057842048, 0.50166852057842048, 0.50166852057842048, None)\n",
      "[[ 65  71  39]\n",
      " [187 339 226]\n",
      " [101 272 498]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "print classification_report(y_pred, y_text_test)\n",
    "print precision_recall_fscore_support(y_pred, y_text_test, average=\"micro\")\n",
    "print confusion_matrix(y_pred, y_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.22      0.31      0.26       250\n",
      "    neutral       0.46      0.45      0.46       699\n",
      "   positive       0.63      0.57      0.60       849\n",
      "\n",
      "avg / total       0.51      0.48      0.49      1798\n",
      "\n",
      "(0.4849833147942158, 0.4849833147942158, 0.4849833147942158, None)\n",
      "[[ 77  99  74]\n",
      " [175 315 209]\n",
      " [101 268 480]]\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(multi_class=\"crammer_singer\")\n",
    "svc.fit(X_train, y_text_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print classification_report(y_pred, y_text_test)\n",
    "print precision_recall_fscore_support(y_pred, y_text_test, average=\"micro\")\n",
    "print confusion_matrix(y_pred, y_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5366, 1394) (1798, 1394)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.12      0.37      0.18       110\n",
      "    neutral       0.39      0.43      0.41       624\n",
      "   positive       0.67      0.48      0.56      1064\n",
      "\n",
      "avg / total       0.54      0.46      0.49      1798\n",
      "\n",
      "(0.45717463848720802, 0.45717463848720802, 0.45717463848720802, None)\n",
      "[[ 41  43  26]\n",
      " [134 267 223]\n",
      " [178 372 514]]\n"
     ]
    }
   ],
   "source": [
    "randF = RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=1337, warm_start=True, class_weight=\"auto\")\n",
    "randF.fit(X_train, y_text_train)\n",
    "\n",
    "print X_train.shape, X_test.shape\n",
    "y_pred = randF.predict(X_test)\n",
    "print classification_report(y_pred, y_text_test)\n",
    "print precision_recall_fscore_support(y_pred, y_text_test, average=\"micro\")\n",
    "print confusion_matrix(y_pred, y_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = randF.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randF = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(filename, char_dict= char_dict, idx = [1,2], label_dict = None, padding=140):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    idx = [label_idx, text_idx, other_idx]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename) as fp:\n",
    "        for line in fp.readlines():\n",
    "            line = line[:-1].split(\"\\t\")\n",
    "            X.append([char_dict.get(k, -1) + 2 for k in line[idx[1]]]) # Add offset of 2, 0 for padding and 1 for OOV\n",
    "            if label_dict is not None:\n",
    "                y = [0]* len(label_dict)                \n",
    "                y[label_dict.get(line[idx[0]])] = 1\n",
    "            else:\n",
    "                y = int(line[idx[0]])\n",
    "            Y.append(y)\n",
    "    X = pad_sequences(X, maxlen=padding)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, Y = get_data(\"data/processed/gold/train/100_topics_100_tweets.sentence-three-point.subtask-A.train.gold.txt\", label_dict=label_dict)\n",
    "X_test, Y_test = get_data(\"data/processed/gold/dev/100_topics_100_tweets.sentence-three-point.subtask-A.dev.gold.txt\", label_dict=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(char_vocab_size, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution1D(nb_filter=10,filter_length=50, border_mode='valid', activation='relu', subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "\"\"\"\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "\"\"\"\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(label_dict), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 3s - loss: 1.0097 - acc: 0.5030 - val_loss: 1.0628 - val_acc: 0.4220\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9870 - acc: 0.5155 - val_loss: 1.0588 - val_acc: 0.4220\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9804 - acc: 0.5160 - val_loss: 1.0627 - val_acc: 0.4220\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9739 - acc: 0.5175 - val_loss: 1.0503 - val_acc: 0.4220\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9653 - acc: 0.5170 - val_loss: 1.0688 - val_acc: 0.4220\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9569 - acc: 0.5185 - val_loss: 1.0476 - val_acc: 0.4270\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9507 - acc: 0.5277 - val_loss: 1.0546 - val_acc: 0.4310\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9457 - acc: 0.5275 - val_loss: 1.0461 - val_acc: 0.4380\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9403 - acc: 0.5372 - val_loss: 1.0480 - val_acc: 0.4325\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 3s - loss: 0.9380 - acc: 0.5345 - val_loss: 1.0487 - val_acc: 0.4345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c34267890>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y, validation_data=(X_test, Y_test), batch_size=128, nb_epoch=10, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(embeddings)\n",
    "plt.yticks(range(97), [\"pad\", \"oov\"] + index_char)\n",
    "plt.ylabel(\"Chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_id = ord('A')\n",
    "z_id = ord('Z')\n",
    "for i in range(a_id, z_id + 1):\n",
    "    print i, chr(i), chr(i) in index_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_embed = dict((k, e) for k, e in zip([\"pad\", \"oov\"] + index_char, embeddings.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_keys = sorted(char_embed.keys())\n",
    "sort_embeddings = np.array([char_embed.get(k) for k in sorted_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(sort_embeddings)\n",
    "plt.yticks(range(len(sorted_keys)), sorted_keys)\n",
    "plt.ylabel(\"Chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
